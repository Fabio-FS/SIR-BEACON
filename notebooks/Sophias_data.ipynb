{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vacc_self  vacc_others01  vacc_others02  vacc_others03  vacc_others04  \\\n",
      "0        5.0              0              0              0              0   \n",
      "1        5.0              0              0              9              0   \n",
      "2        5.0              0              0             48              0   \n",
      "3        5.0             25             15             10              0   \n",
      "4        5.0              0              0              0            100   \n",
      "\n",
      "   vacc_others05  masks_self  masks_others_never  masks_others_sometimes  \\\n",
      "0            100         5.0                   0                       0   \n",
      "1             91         5.0                   0                       0   \n",
      "2             52         5.0                   4                       0   \n",
      "3             50         5.0                   6                      15   \n",
      "4              0         5.0                   0                       0   \n",
      "\n",
      "   masks_others_half  masks_others_often  masks_others_always  testing_self  \\\n",
      "0                  0                   0                  100           2.0   \n",
      "1                  0                   0                  100           1.0   \n",
      "2                 35                   0                   61           2.0   \n",
      "3                 24                  15                   40           1.0   \n",
      "4                  0                 100                    0           4.0   \n",
      "\n",
      "   testing_others_never  testing_others_sometimes  testing_others_half  \\\n",
      "0                   100                         0                    0   \n",
      "1                    96                         4                    0   \n",
      "2                     2                         0                    0   \n",
      "3                    38                         7                   14   \n",
      "4                     0                         0                    0   \n",
      "\n",
      "   testing_others_often  testing_others_always  \n",
      "0                     0                      0  \n",
      "1                     0                      0  \n",
      "2                     0                     98  \n",
      "3                    24                     17  \n",
      "4                   100                      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_homophily.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, Literal\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_behavior_group_counts(\n",
    "    df: pd.DataFrame,\n",
    "    behavior_type: Literal[\"vacc\", \"masks\", \"testing\"]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get the count of individuals in each behavior group.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing survey data\n",
    "        behavior_type: Type of behavior (\"vacc\", \"masks\", or \"testing\")\n",
    "        \n",
    "    Returns:\n",
    "        Array of counts for each behavior group (5 bins)\n",
    "    \"\"\"\n",
    "    # Clean data and prepare\n",
    "    df_clean = df.dropna(subset=[f'{behavior_type}_self'])\n",
    "    n_bins = 5\n",
    "    \n",
    "    # Get self-reported behavior\n",
    "    self_behavior = df_clean[f'{behavior_type}_self'].astype(int)\n",
    "    \n",
    "    # Count individuals per self-reported group\n",
    "    group_counts = np.zeros(n_bins)\n",
    "    for i in range(n_bins):\n",
    "        group_counts[i] = np.sum(self_behavior == i+1)\n",
    "    \n",
    "    return group_counts\n",
    "\n",
    "def calculate_polarization_with_error(\n",
    "    behavior_values: np.ndarray, \n",
    "    behavior_positions: Optional[np.ndarray] = None,\n",
    "    n_bootstrap: int = 1000,\n",
    "    confidence_level: float = 0.95\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate polarization (4 * variance) and its error from behavior values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    behavior_values : array-like\n",
    "        Values representing counts or raw frequencies at each behavior level.\n",
    "        Will be normalized within the function.\n",
    "    behavior_positions : array-like, optional\n",
    "        Positions corresponding to each behavior value.\n",
    "        If None, positions will be evenly spaced from 0 to 1.\n",
    "    n_bootstrap : int, default=1000\n",
    "        Number of bootstrap samples to use for error estimation.\n",
    "    confidence_level : float, default=0.95\n",
    "        Confidence level for the error bounds.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing:\n",
    "        - mean: the mean of the distribution\n",
    "        - variance: the variance of the distribution\n",
    "        - polarization: 4 * variance\n",
    "        - polarization_std_error: standard error of polarization\n",
    "        - polarization_ci: confidence interval for polarization\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array\n",
    "    values = np.array(behavior_values, dtype=float)\n",
    "    \n",
    "    # Normalize values to probabilities\n",
    "    total = np.sum(values)\n",
    "    if total <= 0:\n",
    "        raise ValueError(\"behavior_values must sum to a positive value\")\n",
    "    prob_values = values / total\n",
    "    \n",
    "    # If positions not provided, create evenly spaced positions from 0 to 1\n",
    "    if behavior_positions is None:\n",
    "        positions = np.linspace(0, 1, len(values))\n",
    "    else:\n",
    "        positions = np.array(behavior_positions)\n",
    "    \n",
    "    # Ensure the arrays are the same length\n",
    "    if len(positions) != len(values):\n",
    "        raise ValueError(\"behavior_values and behavior_positions must have the same length\")\n",
    "    \n",
    "    # Function to calculate polarization from probability values\n",
    "    def calc_pol(probs):\n",
    "        mean = np.sum(positions * probs)\n",
    "        variance = np.sum(probs * (positions - mean)**2)\n",
    "        return 4 * variance\n",
    "    \n",
    "    # Calculate polarization for the original data\n",
    "    polarization = calc_pol(prob_values)\n",
    "    \n",
    "    # Bootstrap to estimate error\n",
    "    bootstrap_polarizations = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Generate bootstrap sample (multinomial draw)\n",
    "        bootstrap_sample = np.random.multinomial(total, prob_values, size=1)[0]\n",
    "        bootstrap_probs = bootstrap_sample / np.sum(bootstrap_sample)\n",
    "        bootstrap_polarizations.append(calc_pol(bootstrap_probs))\n",
    "    \n",
    "    # Calculate statistics from bootstrap samples\n",
    "    bootstrap_polarizations = np.array(bootstrap_polarizations)\n",
    "    polarization_std_error = np.std(bootstrap_polarizations, ddof=1)\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = alpha / 2 * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    polarization_ci = np.percentile(bootstrap_polarizations, [lower_percentile, upper_percentile])\n",
    "    \n",
    "    # Calculate mean and variance for original data\n",
    "    mean = np.sum(positions * prob_values)\n",
    "    variance = np.sum(prob_values * (positions - mean)**2)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'variance': variance,\n",
    "        'polarization': polarization,\n",
    "        'polarization_std_error': polarization_std_error,\n",
    "        'polarization_ci': polarization_ci\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_homophily.csv')\n",
    "\n",
    "mask_counts = get_behavior_group_counts(df, \"masks\")\n",
    "results = calculate_polarization_with_error(mask_counts)\n",
    "print(f\"Polarization: {results['polarization']:.4f} ± {results['polarization_std_error']:.4f}\")\n",
    "print(f\"95% CI: [{results['polarization_ci'][0]:.4f}, {results['polarization_ci'][1]:.4f}]\")\n",
    "\n",
    "testing_counts = get_behavior_group_counts(df, \"testing\")\n",
    "results = calculate_polarization_with_error(testing_counts)\n",
    "print(f\"Polarization: {results['polarization']:.4f} ± {results['polarization_std_error']:.4f}\")\n",
    "print(f\"95% CI: [{results['polarization_ci'][0]:.4f}, {results['polarization_ci'][1]:.4f}]\")\n",
    "\n",
    "df = pd.read_csv('data_homophily.csv')\n",
    "vacc_counts = get_behavior_group_counts(df, \"vacc\")\n",
    "results = calculate_polarization_with_error(vacc_counts)\n",
    "print(f\"Polarization: {results['polarization']:.4f} ± {results['polarization_std_error']:.4f}\")\n",
    "print(f\"95% CI: [{results['polarization_ci'][0]:.4f}, {results['polarization_ci'][1]:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List, Literal\n",
    "from scipy.interpolate import interp1d\n",
    "from src.utils.Contact_Matrix import create_contact_matrix, matrix_scaling\n",
    "\n",
    "def generate_matrices_and_vectors(\n",
    "    df: pd.DataFrame,\n",
    "    apply_sinkhorn: bool = True,\n",
    "    symmetrize: bool = True,\n",
    "    max_iters: int = 1000,\n",
    "    threshold: float = 1e-6\n",
    ") -> Tuple[Dict[str, jnp.ndarray], Dict[str, jnp.ndarray]]:\n",
    "    \"\"\"\n",
    "    Generate Sinkhorn-normalized contact matrices and behavior vectors for mask, test, and vaccine.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing survey data\n",
    "        apply_sinkhorn: Whether to apply Sinkhorn normalization\n",
    "        symmetrize: Whether to symmetrize the matrix before normalization\n",
    "        max_iters: Maximum iterations for Sinkhorn normalization\n",
    "        threshold: Convergence threshold for Sinkhorn normalization\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - Dictionary of Sinkhorn-normalized contact matrices for each behavior type\n",
    "        - Dictionary of behavior vectors for each behavior type\n",
    "    \"\"\"\n",
    "    # Define behavior types in the desired order: mask, test, vaccine\n",
    "    behavior_types = [\"masks\", \"testing\", \"vacc\"]\n",
    "    matrices = {}\n",
    "    behavior_vectors = {}\n",
    "    n_bins = 5\n",
    "    \n",
    "    for behavior_type in behavior_types:\n",
    "        # Clean data and prepare\n",
    "        df_clean = df.dropna(subset=[f'{behavior_type}_self'])\n",
    "        \n",
    "        # Get self-reported behavior and calculate behavior vector\n",
    "        self_behavior = df_clean[f'{behavior_type}_self'].astype(int)\n",
    "        behavior_vector = np.zeros(n_bins)\n",
    "        for i in range(n_bins):\n",
    "            behavior_vector[i] = np.sum(self_behavior == i+1)\n",
    "        behavior_vector = behavior_vector / np.sum(behavior_vector)  # Normalize\n",
    "        behavior_vectors[behavior_type] = jnp.array(behavior_vector)\n",
    "        \n",
    "        # Initialize contact matrix\n",
    "        contact_matrix = np.zeros((n_bins, n_bins))\n",
    "        \n",
    "        # Fill contact matrix based on behavior type\n",
    "        if behavior_type == \"vacc\":\n",
    "            cols = [f'{behavior_type}_others0{i+1}' for i in range(n_bins)]\n",
    "        else:\n",
    "            cols = [\n",
    "                f'{behavior_type}_others_never', \n",
    "                f'{behavior_type}_others_sometimes',\n",
    "                f'{behavior_type}_others_half',\n",
    "                f'{behavior_type}_others_often',\n",
    "                f'{behavior_type}_others_always'\n",
    "            ]\n",
    "        \n",
    "        # Aggregate responses into contact matrix\n",
    "        for i, row in df_clean.iterrows():\n",
    "            self_idx = int(row[f'{behavior_type}_self']) - 1\n",
    "            for j, col in enumerate(cols):\n",
    "                if pd.notna(row[col]):\n",
    "                    contact_matrix[self_idx, j] += row[col]\n",
    "        \n",
    "        # Normalize by group counts to get average connections per individual\n",
    "        raw_normalized = np.zeros_like(contact_matrix)\n",
    "        for i in range(n_bins):\n",
    "            group_count = np.sum(self_behavior == i+1)\n",
    "            if group_count > 0:\n",
    "                raw_normalized[i, :] = contact_matrix[i, :] / group_count\n",
    "        \n",
    "        # Symmetrize if requested\n",
    "        if symmetrize:\n",
    "            raw_normalized = 0.5 * (raw_normalized + raw_normalized.T)\n",
    "        \n",
    "        # Convert to JAX array\n",
    "        raw_normalized_jax = jnp.array(raw_normalized)\n",
    "        \n",
    "        # Apply Sinkhorn normalization if requested\n",
    "        if apply_sinkhorn:\n",
    "            sinkhorn_normalized, _, _ = matrix_scaling(\n",
    "                raw_normalized_jax, \n",
    "                max_iters=max_iters, \n",
    "                threshold=threshold\n",
    "            )\n",
    "            # Apply scaling after normalization\n",
    "            matrices[behavior_type] = sinkhorn_normalized * (n_bins**2)\n",
    "        else:\n",
    "            matrices[behavior_type] = raw_normalized_jax * (n_bins**2)\n",
    "    \n",
    "    return matrices, behavior_vectors\n",
    "\n",
    "def calculate_optimal_homophily(\n",
    "    matrices: Dict[str, jnp.ndarray],\n",
    "    behavior_vectors: Dict[str, jnp.ndarray],\n",
    "    h_range: Tuple[float, float] = (-10, 10),\n",
    "    n_samples: int = 1000\n",
    ") -> Tuple[Dict[str, float], Dict[str, Dict[float, float]]]:\n",
    "    \"\"\"\n",
    "    Calculate the optimal homophily parameter and matrix distances for each behavior type.\n",
    "    \n",
    "    Args:\n",
    "        matrices: Dictionary of empirical contact matrices\n",
    "        behavior_vectors: Dictionary of behavior vectors\n",
    "        h_range: Range limits for homophily values\n",
    "        n_samples: Number of homophily values to sample\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - Dictionary of optimal homophily values for each behavior type\n",
    "        - Dictionary of dictionaries containing distance metrics for each tested homophily value\n",
    "    \"\"\"\n",
    "    # Define behavior types in the desired order: mask, test, vaccine\n",
    "    behavior_types = [\"masks\", \"testing\", \"vacc\"]\n",
    "    h_values = jnp.linspace(h_range[0], h_range[1], n_samples)\n",
    "    optimal_h = {}\n",
    "    all_distances = {}\n",
    "    \n",
    "    # Function to calculate distance between matrices\n",
    "    def matrix_distance(m1, m2):\n",
    "        # Frobenius norm (L2 distance)\n",
    "        return jnp.sqrt(jnp.sum((m1 - m2) ** 2))\n",
    "    \n",
    "    for behavior_type in behavior_types:\n",
    "        empirical_matrix = matrices[behavior_type]\n",
    "        populations = behavior_vectors[behavior_type]\n",
    "        n_groups = len(populations)\n",
    "        \n",
    "        # Calculate distance for each homophily value\n",
    "        distances = {}\n",
    "        \n",
    "        for h in h_values:\n",
    "            synthetic_matrix = create_contact_matrix(n_groups, h, populations)\n",
    "            dist = matrix_distance(synthetic_matrix, empirical_matrix)\n",
    "            distances[float(h)] = float(dist)\n",
    "        \n",
    "        # Find optimal homophily\n",
    "        optimal_h[behavior_type] = min(distances, key=distances.get)\n",
    "        all_distances[behavior_type] = distances\n",
    "    \n",
    "    return optimal_h, all_distances\n",
    "\n",
    "def calculate_h_std_dev(\n",
    "    distances: Dict[float, float],\n",
    "    temperature: float = 1.0\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate the optimal homophily value and its standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        distances: Dictionary mapping h values to their L2 distances\n",
    "        temperature: Temperature parameter for softening the distribution\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - Optimal h value (minimum distance)\n",
    "        - Standard deviation of h\n",
    "        - Effective sample size\n",
    "    \"\"\"\n",
    "    # Convert distances to arrays\n",
    "    h_values = np.array(sorted(distances.keys()))\n",
    "    dist_values = np.array([distances[h] for h in h_values])\n",
    "    \n",
    "    # Find the optimal h (minimum distance)\n",
    "    min_idx = np.argmin(dist_values)\n",
    "    optimal_h = h_values[min_idx]\n",
    "    \n",
    "    # Convert distances to weights (higher weight = lower distance)\n",
    "    # Use softmax-like transformation with temperature parameter\n",
    "    weights = np.exp(-dist_values / temperature)\n",
    "    \n",
    "    # Normalize weights to sum to 1 (probability distribution)\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Calculate weighted mean (should be close to optimal_h)\n",
    "    weighted_mean = np.sum(h_values * weights)\n",
    "    \n",
    "    # Calculate weighted variance\n",
    "    variance = np.sum(weights * (h_values - weighted_mean)**2)\n",
    "    \n",
    "    # Standard deviation\n",
    "    std_dev = np.sqrt(variance)\n",
    "    \n",
    "    # Calculate effective sample size (lower when distribution is more spread out)\n",
    "    effective_n = 1.0 / np.sum(weights**2)\n",
    "    \n",
    "    return optimal_h, std_dev, effective_n\n",
    "\n",
    "def generate_comparison_figure_with_std(\n",
    "    df: pd.DataFrame,\n",
    "    matrices: Dict[str, jnp.ndarray],\n",
    "    behavior_vectors: Dict[str, jnp.ndarray],\n",
    "    distances: Dict[str, Dict[float, float]],\n",
    "    optimal_h: Dict[str, float],\n",
    "    figsize: Tuple[int, int] = (15, 12)\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Generate a 3x3 comparison figure for mask, test, and vaccine data,\n",
    "    including optimal h and standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing survey data\n",
    "        matrices: Dictionary of contact matrices\n",
    "        behavior_vectors: Dictionary of behavior vectors\n",
    "        distances: Dictionary of dictionaries with distance metrics\n",
    "        optimal_h: Dictionary of optimal homophily values\n",
    "        figsize: Figure size\n",
    "        \n",
    "    Returns:\n",
    "        Matplotlib Figure object\n",
    "    \"\"\"\n",
    "    #Mh = np.array([2.2041, 2.4490, 2.7551])\n",
    "    #Th = np.array([2.1429, 2.3265, 2.5102])\n",
    "    #Vh = np.array([1.6515, 1.7755, 1.8980])\n",
    "\n",
    "\n",
    "    # Define behavior types in the desired order: mask, test, vaccine\n",
    "    behavior_types = [\"masks\", \"testing\", \"vacc\"]\n",
    "    behavior_labels = [\"Masks\", \"Testing\", \"Vaccination\"]\n",
    "    \n",
    "    # Calculate standard deviation for each behavior type\n",
    "    std_devs = {}\n",
    "    for behavior_type in behavior_types:\n",
    "        optimal_h_val, std_dev, effective_n = calculate_h_std_dev(distances[behavior_type])\n",
    "        std_devs[behavior_type] = {\n",
    "            'optimal': optimal_h_val,\n",
    "            'std_dev': std_dev,\n",
    "            'effective_n': effective_n\n",
    "        }\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(3, 3, figsize=figsize)\n",
    "    \n",
    "    # Plot data for each behavior type\n",
    "    for col, (behavior_type, label) in enumerate(zip(behavior_types, behavior_labels)):\n",
    "        # Top row: Histogram of self-reported behavior\n",
    "        ax_hist = axes[0, col]\n",
    "        behavior_vector = behavior_vectors[behavior_type]\n",
    "        ax_hist.bar(range(1, len(behavior_vector)+1), behavior_vector)\n",
    "        ax_hist.set_title(f\"{label} Self-Reported Behavior\")\n",
    "        ax_hist.set_xlabel(\"Behavior Level\")\n",
    "        ax_hist.set_ylabel(\"Proportion\")\n",
    "        ax_hist.set_xticks(range(1, len(behavior_vector)+1))\n",
    "        ax_hist.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Middle row: Normalized contact matrix\n",
    "        ax_matrix = axes[1, col]\n",
    "        im = ax_matrix.imshow(matrices[behavior_type], cmap='viridis')\n",
    "        ax_matrix.set_title(f\"{label} Contact Matrix\")\n",
    "        ax_matrix.set_xlabel(\"Others' Behavior\")\n",
    "        ax_matrix.set_ylabel(\"Self Behavior\")\n",
    "        ax_matrix.set_xticks(range(len(behavior_vector)))\n",
    "        ax_matrix.set_yticks(range(len(behavior_vector)))\n",
    "        ax_matrix.set_xticklabels(range(1, len(behavior_vector)+1))\n",
    "        ax_matrix.set_yticklabels(range(1, len(behavior_vector)+1))\n",
    "        plt.colorbar(im, ax=ax_matrix)\n",
    "        \n",
    "        # Bottom row: L2 distance curve with standard deviation\n",
    "        ax_dist = axes[2, col]\n",
    "        h_vals = sorted(distances[behavior_type].keys())\n",
    "        d_vals = [distances[behavior_type][h] for h in h_vals]\n",
    "        ax_dist.plot(h_vals, d_vals, '-', linewidth=2)\n",
    "        \n",
    "        # Add optimal h and standard deviation\n",
    "        std_data = std_devs[behavior_type]\n",
    "        optimal_h_val = std_data['optimal']\n",
    "        std_dev = std_data['std_dev']\n",
    "        \n",
    "        # Plot optimal h\n",
    "        ax_dist.axvline(x=optimal_h_val, color='r', linestyle='-', \n",
    "                       label=f'Optimal h = {optimal_h_val:.2f}')\n",
    "        \n",
    "        # Add standard deviation range as shaded area\n",
    "        ax_dist.axvspan(\n",
    "            optimal_h_val - std_dev, \n",
    "            optimal_h_val + std_dev, \n",
    "            alpha=0.2, \n",
    "            color='blue',\n",
    "            label=f'h ± σ: [{optimal_h_val-std_dev:.2f}, {optimal_h_val+std_dev:.2f}]'\n",
    "        )\n",
    "        \n",
    "        # Add vertical lines for std dev bounds\n",
    "        ax_dist.axvline(x=optimal_h_val - std_dev, color='blue', linestyle='--', alpha=0.5)\n",
    "        ax_dist.axvline(x=optimal_h_val + std_dev, color='blue', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        ax_dist.set_xlabel(\"Homophily Parameter (h)\")\n",
    "        ax_dist.set_ylabel(\"Matrix Distance\")\n",
    "        ax_dist.set_title(f\"{label} Matrix Distance\")\n",
    "        ax_dist.legend()\n",
    "        ax_dist.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_behavior_homophily_with_std(\n",
    "    csv_path: str = 'data_homophily.csv',\n",
    "    n_samples: int = 1000,\n",
    "    h_range: Tuple[float, float] = (-10, 10),\n",
    "    save_figure: bool = False,\n",
    "    figure_path: str = 'homophily_analysis_with_std.png'\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze homophily for all behavior types and generate comparison figure with standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the CSV file with survey data\n",
    "        n_samples: Number of homophily values to sample\n",
    "        h_range: Range of homophily values to test\n",
    "        save_figure: Whether to save the figure to file\n",
    "        figure_path: Path to save the figure\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of results including optimal homophily values, standard deviations, and figure\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Generate matrices and behavior vectors\n",
    "    matrices, behavior_vectors = generate_matrices_and_vectors(df)\n",
    "    \n",
    "    # Calculate optimal homophily and distances\n",
    "    optimal_h, distances = calculate_optimal_homophily(\n",
    "        matrices, \n",
    "        behavior_vectors, \n",
    "        h_range=h_range,\n",
    "        n_samples=n_samples\n",
    "    )\n",
    "    \n",
    "    # Calculate standard deviations for each behavior type\n",
    "    std_devs = {}\n",
    "    for behavior_type in [\"masks\", \"testing\", \"vacc\"]:\n",
    "        optimal_h_val, std_dev, effective_n = calculate_h_std_dev(distances[behavior_type])\n",
    "        std_devs[behavior_type] = {\n",
    "            'optimal': optimal_h_val,\n",
    "            'std_dev': std_dev,\n",
    "            'effective_n': effective_n\n",
    "        }\n",
    "    \n",
    "    # Generate comparison figure with standard deviation\n",
    "    fig = generate_comparison_figure_with_std(\n",
    "        df, \n",
    "        matrices, \n",
    "        behavior_vectors, \n",
    "        distances, \n",
    "        optimal_h\n",
    "    )\n",
    "    \n",
    "    # Print optimal homophily values and standard deviations\n",
    "    print(\"Homophily Parameter Estimates with Standard Deviations:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Behavior':<12} {'Optimal h':<12} {'Std Dev (σ)':<12} {'h ± σ':<20} {'Effective N':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for behavior_type, label in zip([\"masks\", \"testing\", \"vacc\"], [\"Masks\", \"Testing\", \"Vaccination\"]):\n",
    "        std_data = std_devs[behavior_type]\n",
    "        optimal_h_val = std_data['optimal']\n",
    "        std_dev = std_data['std_dev']\n",
    "        effective_n = std_data['effective_n']\n",
    "        h_range_str = f\"[{optimal_h_val-std_dev:.4f}, {optimal_h_val+std_dev:.4f}]\"\n",
    "        \n",
    "        print(f\"{label:<12} {optimal_h_val:<12.4f} {std_dev:<12.4f} {h_range_str:<20} {effective_n:<12.1f}\")\n",
    "    \n",
    "    # Save figure if requested\n",
    "    if save_figure:\n",
    "        fig.savefig(figure_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {figure_path}\")\n",
    "    \n",
    "    # Display figure\n",
    "    plt.show()\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'matrices': matrices,\n",
    "        'behavior_vectors': behavior_vectors,\n",
    "        'optimal_h': optimal_h,\n",
    "        'distances': distances,\n",
    "        'std_devs': std_devs,\n",
    "        'figure': fig\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# results = analyze_behavior_homophily_with_std('data_homophily.csv')\n",
    "\n",
    "results = analyze_behavior_homophily_with_std(\n",
    "    csv_path='data_homophily.csv',\n",
    "    n_samples=1000,\n",
    "    h_range=(-10, 10),\n",
    "    save_figure=True,\n",
    "    figure_path='homophily_analysis_with_std.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List, Literal, Optional\n",
    "from scipy.interpolate import interp1d\n",
    "from src.utils.Contact_Matrix import create_contact_matrix, matrix_scaling\n",
    "\n",
    "def create_empirical_contact_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    behavior_type: Literal[\"vacc\", \"masks\", \"testing\"],\n",
    "    apply_sinkhorn: bool = True,\n",
    "    symmetrize: bool = True,\n",
    "    max_iters: int = 1000,\n",
    "    threshold: float = 1e-6\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Create contact matrices from survey data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing survey data\n",
    "        behavior_type: Type of behavior (\"vacc\", \"masks\", or \"testing\")\n",
    "        apply_sinkhorn: Whether to apply Sinkhorn normalization\n",
    "        symmetrize: Whether to symmetrize the matrix before normalization\n",
    "        max_iters: Maximum number of iterations for Sinkhorn normalization\n",
    "        threshold: Convergence threshold for Sinkhorn normalization\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - Group counts (population sizes)\n",
    "        - Raw normalized contact matrix\n",
    "        - Sinkhorn-normalized contact matrix (if apply_sinkhorn=True, otherwise same as raw)\n",
    "    \"\"\"\n",
    "    # Clean data and prepare\n",
    "    df_clean = df.dropna(subset=[f'{behavior_type}_self'])\n",
    "    n_bins = 5\n",
    "    \n",
    "    # Initialize contact matrix\n",
    "    contact_matrix = np.zeros((n_bins, n_bins))\n",
    "    \n",
    "    # Get self-reported behavior\n",
    "    self_behavior = df_clean[f'{behavior_type}_self'].astype(int)\n",
    "    \n",
    "    # Count individuals per self-reported group for normalization\n",
    "    group_counts = np.zeros(n_bins)\n",
    "    for i in range(n_bins):\n",
    "        group_counts[i] = np.sum(self_behavior == i+1)\n",
    "    \n",
    "    # Fill contact matrix based on behavior type\n",
    "    if behavior_type == \"vacc\":\n",
    "        cols = [f'{behavior_type}_others0{i+1}' for i in range(n_bins)]\n",
    "    else:\n",
    "        cols = [\n",
    "            f'{behavior_type}_others_never', \n",
    "            f'{behavior_type}_others_sometimes',\n",
    "            f'{behavior_type}_others_half',\n",
    "            f'{behavior_type}_others_often',\n",
    "            f'{behavior_type}_others_always'\n",
    "        ]\n",
    "    \n",
    "    # Aggregate responses into contact matrix\n",
    "    for i, row in df_clean.iterrows():\n",
    "        self_idx = int(row[f'{behavior_type}_self']) - 1\n",
    "        for j, col in enumerate(cols):\n",
    "            if pd.notna(row[col]):\n",
    "                contact_matrix[self_idx, j] += row[col]\n",
    "    \n",
    "    # Normalize by group counts to get average connections per individual\n",
    "    raw_normalized = np.zeros_like(contact_matrix)\n",
    "    for i in range(n_bins):\n",
    "        if group_counts[i] > 0:\n",
    "            raw_normalized[i, :] = contact_matrix[i, :] / group_counts[i]\n",
    "    \n",
    "    # Symmetrize if requested\n",
    "    if symmetrize:\n",
    "        raw_normalized = 0.5 * (raw_normalized + raw_normalized.T)\n",
    "    \n",
    "    # Convert to JAX array\n",
    "    raw_normalized_jax = jnp.array(raw_normalized)\n",
    "    \n",
    "    # Apply Sinkhorn normalization if requested\n",
    "    if apply_sinkhorn:\n",
    "        sinkhorn_normalized, _, _ = matrix_scaling(\n",
    "            raw_normalized_jax, \n",
    "            max_iters=max_iters, \n",
    "            threshold=threshold\n",
    "        )\n",
    "        return jnp.array(group_counts), raw_normalized_jax, sinkhorn_normalized\n",
    "    else:\n",
    "        return jnp.array(group_counts), raw_normalized_jax, raw_normalized_jax\n",
    "\n",
    "def calculate_optimal_homophily(\n",
    "    empirical_matrix: jnp.ndarray,\n",
    "    populations: jnp.ndarray,\n",
    "    h_range: Tuple[float, float] = (-10, 10),\n",
    "    n_samples: int = 1000\n",
    ") -> Tuple[float, Dict[float, float]]:\n",
    "    \"\"\"\n",
    "    Calculate the optimal homophily parameter and matrix distances.\n",
    "    \n",
    "    Args:\n",
    "        empirical_matrix: Empirical contact matrix\n",
    "        populations: Population sizes for each behavior group\n",
    "        h_range: Range limits for homophily values\n",
    "        n_samples: Number of homophily values to sample\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - Optimal homophily value\n",
    "        - Dictionary mapping h values to distances\n",
    "    \"\"\"\n",
    "    h_values = np.linspace(h_range[0], h_range[1], n_samples)\n",
    "    n_groups = len(populations)\n",
    "    distances = {}\n",
    "    \n",
    "    # Function to calculate distance between matrices\n",
    "    def matrix_distance(m1, m2):\n",
    "        # Frobenius norm (L2 distance)\n",
    "        return jnp.sqrt(jnp.sum((m1 - m2) ** 2))\n",
    "    \n",
    "    for h in h_values:\n",
    "        synthetic_matrix = create_contact_matrix(n_groups, h, populations)\n",
    "        dist = float(matrix_distance(synthetic_matrix, empirical_matrix))\n",
    "        distances[float(h)] = dist\n",
    "    \n",
    "    # Find optimal homophily\n",
    "    optimal_h = min(distances, key=distances.get)\n",
    "    \n",
    "    return optimal_h, distances\n",
    "\n",
    "def bootstrap_homophily_estimation(\n",
    "    df: pd.DataFrame,\n",
    "    behavior_type: Literal[\"vacc\", \"masks\", \"testing\"],\n",
    "    n_bootstrap: int = 1000,\n",
    "    h_range: Tuple[float, float] = (-10, 10),\n",
    "    n_h_samples: int = 200,\n",
    "    confidence_level: float = 0.95,\n",
    "    apply_sinkhorn: bool = True,\n",
    "    symmetrize: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Estimate homophily parameter with bootstrap confidence intervals.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing survey data\n",
    "        behavior_type: Type of behavior (\"vacc\", \"masks\", or \"testing\")\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "        h_range: Range of homophily values to test\n",
    "        n_h_samples: Number of homophily values to sample within the range\n",
    "        confidence_level: Confidence level for intervals\n",
    "        apply_sinkhorn: Whether to apply Sinkhorn normalization\n",
    "        symmetrize: Whether to symmetrize matrices\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results including optimal h, standard error, and confidence intervals\n",
    "    \"\"\"\n",
    "    # Get original matrix and population counts\n",
    "    group_counts, _, original_matrix = create_empirical_contact_matrix(\n",
    "        df, \n",
    "        behavior_type, \n",
    "        apply_sinkhorn=apply_sinkhorn, \n",
    "        symmetrize=symmetrize\n",
    "    )\n",
    "    \n",
    "    # Scale matrix by population size to match expected behavior\n",
    "    n_bins = len(group_counts)\n",
    "    original_matrix = original_matrix * (n_bins**2)\n",
    "    \n",
    "    # Calculate optimal homophily on original data\n",
    "    original_optimal_h, original_distances = calculate_optimal_homophily(\n",
    "        original_matrix,\n",
    "        group_counts,\n",
    "        h_range=h_range,\n",
    "        n_samples=n_h_samples\n",
    "    )\n",
    "    \n",
    "    # Prepare for bootstrap\n",
    "    bootstrap_h_values = []\n",
    "    \n",
    "    # Get relevant columns based on behavior type\n",
    "    if behavior_type == \"vacc\":\n",
    "        behavior_cols = [f'{behavior_type}_self'] + [f'{behavior_type}_others0{i+1}' for i in range(n_bins)]\n",
    "    else:\n",
    "        behavior_cols = [f'{behavior_type}_self', \n",
    "                        f'{behavior_type}_others_never', \n",
    "                        f'{behavior_type}_others_sometimes',\n",
    "                        f'{behavior_type}_others_half',\n",
    "                        f'{behavior_type}_others_often',\n",
    "                        f'{behavior_type}_others_always']\n",
    "    \n",
    "    # Clean data\n",
    "    df_clean = df.dropna(subset=[f'{behavior_type}_self'])\n",
    "    \n",
    "    # Perform bootstrap resampling\n",
    "    for i in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        bootstrap_indices = np.random.choice(len(df_clean), size=len(df_clean), replace=True)\n",
    "        bootstrap_sample = df_clean.iloc[bootstrap_indices].copy()\n",
    "        \n",
    "        # Create empirical matrix from bootstrap sample\n",
    "        bs_group_counts, _, bs_matrix = create_empirical_contact_matrix(\n",
    "            bootstrap_sample,\n",
    "            behavior_type,\n",
    "            apply_sinkhorn=apply_sinkhorn,\n",
    "            symmetrize=symmetrize\n",
    "        )\n",
    "        \n",
    "        # Scale matrix\n",
    "        bs_matrix = bs_matrix * (n_bins**2)\n",
    "        \n",
    "        # Calculate optimal homophily for this bootstrap sample\n",
    "        bs_optimal_h, _ = calculate_optimal_homophily(\n",
    "            bs_matrix,\n",
    "            bs_group_counts,\n",
    "            h_range=h_range,\n",
    "            n_samples=n_h_samples\n",
    "        )\n",
    "        \n",
    "        bootstrap_h_values.append(bs_optimal_h)\n",
    "    \n",
    "    # Calculate bootstrap statistics\n",
    "    bootstrap_h_values = np.array(bootstrap_h_values)\n",
    "    h_mean = np.mean(bootstrap_h_values)\n",
    "    h_std_error = np.std(bootstrap_h_values, ddof=1)\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = alpha / 2 * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    h_ci = np.percentile(bootstrap_h_values, [lower_percentile, upper_percentile])\n",
    "    \n",
    "    return {\n",
    "        'behavior_type': behavior_type,\n",
    "        'optimal_h': original_optimal_h,\n",
    "        'bootstrap_mean_h': h_mean,\n",
    "        'h_std_error': h_std_error,\n",
    "        'h_ci': h_ci,\n",
    "        'bootstrap_samples': bootstrap_h_values,\n",
    "        'distances': original_distances\n",
    "    }\n",
    "\n",
    "def analyze_all_behaviors_bootstrap(\n",
    "    df: pd.DataFrame,\n",
    "    n_bootstrap: int = 1000,\n",
    "    h_range: Tuple[float, float] = (-10, 10),\n",
    "    n_h_samples: int = 200,\n",
    "    confidence_level: float = 0.95,\n",
    "    apply_sinkhorn: bool = True,\n",
    "    symmetrize: bool = True\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Analyze all behavior types using bootstrap for error estimation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing survey data\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "        h_range: Range of homophily values to test\n",
    "        n_h_samples: Number of homophily values to sample within the range\n",
    "        confidence_level: Confidence level for intervals\n",
    "        apply_sinkhorn: Whether to apply Sinkhorn normalization\n",
    "        symmetrize: Whether to symmetrize matrices\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping behavior types to their analysis results\n",
    "    \"\"\"\n",
    "    behavior_types = [\"masks\", \"testing\", \"vacc\"]\n",
    "    results = {}\n",
    "    \n",
    "    for behavior_type in behavior_types:\n",
    "        print(f\"Analyzing {behavior_type}...\")\n",
    "        results[behavior_type] = bootstrap_homophily_estimation(\n",
    "            df,\n",
    "            behavior_type,\n",
    "            n_bootstrap=n_bootstrap,\n",
    "            h_range=h_range,\n",
    "            n_h_samples=n_h_samples,\n",
    "            confidence_level=confidence_level,\n",
    "            apply_sinkhorn=apply_sinkhorn,\n",
    "            symmetrize=symmetrize\n",
    "        )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nHomophily Parameter Estimates with Bootstrap Error Analysis:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Behavior':<12} {'Optimal h':<12} {'Std Error':<12} {f'{confidence_level*100}% CI':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for behavior_type, label in zip(behavior_types, [\"Masks\", \"Testing\", \"Vaccination\"]):\n",
    "        result = results[behavior_type]\n",
    "        optimal_h = result['optimal_h']\n",
    "        h_std_error = result['h_std_error']\n",
    "        h_ci = result['h_ci']\n",
    "        ci_str = f\"[{h_ci[0]:.4f}, {h_ci[1]:.4f}]\"\n",
    "        \n",
    "        print(f\"{label:<12} {optimal_h:<12.4f} {h_std_error:<12.4f} {ci_str:<20}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('data_homophily.csv')\n",
    "\n",
    "# Run the analysis for all behavior types\n",
    "results = analyze_all_behaviors_bootstrap(\n",
    "    df,\n",
    "    n_bootstrap=1000,          # Number of bootstrap samples\n",
    "    h_range=(0, 3),         # Range of homophily values to test\n",
    "    n_h_samples=50,           # Number of h values to sample within range\n",
    "    confidence_level=0.95,     # Confidence level for intervals (95%)\n",
    "    apply_sinkhorn=True,       # Apply Sinkhorn normalization\n",
    "    symmetrize=True           # Don't symmetrize matrices (or set to True if desired)\n",
    ")\n",
    "\n",
    "# You can access individual results for each behavior type\n",
    "masks_results = results['masks']\n",
    "testing_results = results['testing']\n",
    "vacc_results = results['vacc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homophily values for masks, testing, and vaccination SE and 95% CI\n",
    "Mh = np.array([2.2041, 2.4490, 2.7551])\n",
    "Th = np.array([2.1429, 2.3265, 2.5102])\n",
    "Vh = np.array([1.6515, 1.7755, 1.8980])\n",
    "\n",
    "# polarization values for masks, testing, and vaccination\n",
    "Mp = np.array([0.2658, 0.2898, 0.3149])\n",
    "Tp = np.array([0.4575, 0.4780, 0.4981])\n",
    "Vp = np.array([0.5387, 0.5637, 0.5913])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Optimal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
